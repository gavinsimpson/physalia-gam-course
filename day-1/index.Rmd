---
title: "Generalized Additive Models"
subtitle: "a data-driven approach to estimating regression models"
institute: "Department of Animal & Veterinary Sciences · Aarhus University"
author: "Gavin Simpson"
date: "1400&ndash;2000 CET (1300&ndash;1900 UTC) Monday 20th March, 2023"
output:
  xaringan::moon_reader:
    css: ['default', 'https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css', 'slides.css']
    lib_dir: libs
    nature:
      titleSlideClass: ['inverse','middle','left',my-title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
      ratio: '16:9'
---
class: inverse middle center big-subsection

```{r setup, include=FALSE, cache=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, dev = 'svg', echo = TRUE, message = FALSE, warning = FALSE,
                      fig.height = 6, fig.width = 1.777777 * 6)

library('here')
library('mgcv')
library('gratia')
library('ggplot2')
library('purrr')
library('mvnfast')
library("tibble")
library('patchwork')
library('tidyr')
library("knitr")
library("viridis")
library('readr')
library('dplyr')
library('gganimate')

## plot defaults
theme_set(theme_minimal(base_size = 16, base_family = 'Fira Sans'))

## constants
anim_width <- 1000
anim_height <- anim_width / 1.77777777
anim_dev <- 'png'
anim_res <- 200
```

# Welcome

???

---

# Logistics

## Slides

Slidedeck: [bit.ly/physalia-gam-1](https://bit.ly/physalia-gam-1)

Sources: [bit.ly/physalia-gam](https://bit.ly/physalia-gam)

Direct download a ZIP of everything: [bit.ly/physalia-gam-zip](https://bit.ly/physalia-gam-zip)

Unpack the zip & remember where you put it

---

# Today's topics

* Brief overview of R and the Tidyverse packages we’ll encounter throughout the course

* Recap generalised linear models

* Fitting your first GAM

---
class: inverse middle center subsection

# GLMs

---

# Generalized linear models

Generalised linear models (GLMs) are an extension of linear regression plus Poisson, logistic and other regression models

GLMs extend the types of data and error distributions that can be modelled beyond the Gaussian data of linear regression

With GLMs we can model count data, binary/presence absence data, and concentration data where the response variable is not continuous.

Such data have different mean-variance relationships and we would not expect errors to be Gaussian.

---

# Generalized linear models

Typical uses of GLMs in ecology are

- Poisson GLM for count data
- Logistic GLM for presence absence data
- Gamma GLM for non-negative or positive continuous data

GLMs can handle many problems that appear non-linear

Not necessary to transform data as this is handled as part of the GLM process

---

# Binomial distribution

* For a fixed number of trials (*n*),
* fixed probability of “success” (*p*), &
* two outcomes per trial (heads or tails)

Flip a coin 10 times with *p* = 0.7, the probability of 7 heads is $\sim Bin(n = 10, p = 0.7)$, ~ 0.27

```{r}
dbinom(x = 7, size = 10, prob = 0.7)
```

---

# Binomial distribution

```{r binomial-pdf, echo = FALSE}
## Binomial Probability mass function
s <- seq(0, 40, by = 1)
n <- rep(c(20,20,40), each = length(s))
binom.pmf <- data.frame(x = rep(s, 3),
                        n = rep(c(20,20,40), each = length(s)),
                        p = rep(c(0.5, 0.7, 0.5), each = length(s)))
binom.pmf <- transform(binom.pmf,
                       pmf = dbinom(x, size = n, prob = p),
                       params = paste("n=", n, "; p=", p, sep = ""))

plt.binom <- ggplot(binom.pmf, aes(x = x, y = pmf, colour = params)) +
    geom_point() + labs(y = "Probability Mass", x = "No. of Successes")
plt.binom
```

---

# Poisson distribution

The Poisson gives the distribution of the number of “things” (individuals, events, counts) in a given sampling interval/effort if each event is **independent**.

Has a single parameter $\lambda$ the average density or arrival rate

---

# Poisson distribution

```{r poisson-pdf, echo = FALSE}
s <- seq(0, 20, by = 1)
poisson.pmf <- data.frame(x = rep(s, 3),
                          lambda = rep(c(1,4,10), each = length(s)))
poisson.pmf <- transform(poisson.pmf,
                         pmf = dpois(x, lambda = lambda),
                         params = paste("lambda=", lambda, sep = ""))

plt.poisson <- ggplot(poisson.pmf, aes(x = x, y = pmf, colour = params)) +
    geom_point() + labs(y = "Probability Mass", x = "Count")
plt.poisson
```

---

# The structure of a GLM

A GLM consists of three components, chosen/specified by the user

.small[
1. A **Random component**, specifying the conditional distribution of of the response $y_i$ given the values of the explanatory data
2. A **Linear Predictor** $\eta$ &mdash; the linear function of regressors
    $$\eta_i = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}$$
	The $x_{ij}$ are prescribed functions of the explanatory variables and can be transformed variables, dummy variables, polynomial terms, interactions etc.
3. A smooth and invertible **Link Function** $g(\cdot)$, which transforms the expectation of the response $\mu_i \equiv \mathbb{E}(y_i)$ to the linear predictor
    $$g(\mu_i) = \eta_i = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik}$$
    As $g(\cdot)$ is invertible, we can write
    $$\mu_i = g^{-1}(\eta_i) = g^{-1}(\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_k x_{ik})$$
]

---

# Conditional distribution of $y_i$

Originally GLMs were specified for error distribution functions belonging to the *exponential family* of probability distributions

- Continuous probability distributions
    - Gaussian (or normal distribution; used in linear regression)
	- Gamma (data with constant coefficient of variation)
	- Exponential (time to death, survival analysis)
	- Chi-square
	- Inverse-Gaussian
- Discrete probability distributions
    - Poisson (count data)
	- Binomial (0/1 data, counts from a total)
	- Multinomial

Choice depends on range of $y_i$ and on the relationship between the variance and the expectation of $y_i$ &mdash; *mean-variance relationship*

---

# Conditional distribution of $y_i$

Characteristics of common GLM probability distributions


|                  | Canonical Link | Range of $Y_i$               | Variance function              |
|------------------|----------------|------------------------------|--------------------------------|
| Gaussian         | Identity       | $(-\infty, +\infty)$         | $\phi$                         |
| Poisson          | Log            | $0,1,2,\ldots,\infty$        | $\mu_i$                        |
| Binomial         | Logit          | $\frac{0,1,\ldots,n_i}{n_i}$ | $\frac{\mu_i(1 - \mu_i)}{n_i}$ |
| Gamma            | Inverse        | $(0, \infty)$                | $\phi \mu_i^2$                 |
| Inverse-Gaussian | Inverse-square | $(0, \infty)$                | $\phi \mu_i^3$                 |


$\phi$ is the dispersion parameter; $\mu_i$ is the expectation of $y_i$. In the binomial family, $n_i$ is the number of trials

---

# Ecologically-relevant probability distributions

Gaussian distribution is rarely adequate in ecology; GLMs offer ecologically meaningful alternatives

.small[
- **Poisson** &mdash; counts; integers, non-negative, variance increases with mean

- **Binomial** &mdash; observed proportions from a total; integers, non-negative, bounded at 0 and 1, variance largest at $\pi = 0.5$

- **Binomial** &mdash; presence absence data; discrete values, 0 and 1, models probability of success

- **Gamma** &mdash; concentrations; non-negative (strictly positive with log link) real values, variance increases with mean, many zero values and some high values
]

---

# Old notation

Wrote linear model as

$$y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ij} + \varepsilon_i$$

And assumed

$$ \varepsilon_i \sim \text{Normal}(0, \sigma^2) $$

This doesn't work out the same for GLMs &mdash; we don't have residuals in the linear predictor

Sampling variation comes from the response distribution

---

# New notation

Rewrite linear model as

\begin{align*}
y_i & \sim \text{Normal}(\mu_i, \sigma^2) \\
\eta_i & = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ij}
\end{align*}

This now matches the general form for the GLM

\begin{align*}
y_i & \sim \text{EF}(\mu_i, \boldsymbol{\theta}) \\
g(\mu_i) & = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ij}
\end{align*}

---

# New notation

Binomial GLM

\begin{align*}
y_i & \sim \text{Binomial}(n, p_i) \\
\text{logit}(p_i) & = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ij}
\end{align*}

Poisson GLM

\begin{align*}
y_i & \sim \text{Poisson}(\lambda_i) \\
\log(\lambda_i) & = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_j x_{ij}
\end{align*}

---
class: inverse middle center subsection

# Examples

---

# Logistic regression &mdash; *Darlingtonia*

Timed censuses at 42 randomly-chosen leaves of the cobra lily

.small[
* Recorded number of wasp visits at 10 of the 42 leaves
* Test hypothesis that the probability of visitation is related to leaf height
* Response is dichotomous variable (0/1)
* A suitable model is the logistic model
    $$\pi = \frac{e^{\beta_0 + \beta_i X}}{1 + e^{\beta_0 + \beta_1 X_i}}$$
* The logit transformation produces
    $$\log_e \left( \frac{\pi}{1-\pi} \right) = \beta_0 + \beta_1 X_i$$
* This is the logistic regression and it is a special case of the GLM, with a binomial random component and the logit link function
]

---

# Logistic regression &mdash; *Darlingtonia*

$$\log_e \left( \frac{\pi}{1-\pi} \right) = \beta_0 + \beta_1 X_i$$

.small[
* $\beta_0$ is a type of intercept; determines the probability of success ( $y_i = 1$ ) $\pi$ where X = 0
* If $\beta_0 = 0$ then $\pi = 0.5$
*  $\beta_1$ is similar to the slope and determines how steeply the fitted logistic curve rises to the maximum value of $\pi = 1$
* Together, $\beta_0$ and $\beta_1$ specify the range of the $X$ variable over which most of the rise occurs and determine how quickly the probability rises from 0 to 1
* Estimate the model parameters using **Maximum Likelihood**; find parameter values that make the observed data most probable
]

---

# Logistic regression &mdash; *Darlingtonia*

```{r load-darl-data, echo = TRUE}
wasp <- read_csv(here("data", "darlingtonia.csv"), comment = "#",
                 col_types = "dl")
```

---

# Logistic regression &mdash; *Darlingtonia*

```{r fit-darlingtonia, echo = TRUE}
m <- glm(visited ~ leafHeight, data = wasp, family = binomial)
m
```

---

# Logistic regression &mdash; *Darlingtonia*

.smaller[
```{r summary-darlingtonia, echo = TRUE}
summary(m)
```
]

---

# Logistic regression &mdash; *Darlingtonia*

.row[
.col-6[
.smaller[
```{r predict-darlingtonia, echo = TRUE, eval = FALSE}
# data to predict at
pdat <- with(wasp,
             tibble(leafHeight = seq(min(leafHeight),
                                     max(leafHeight),
                                     length = 100)))
# predict
pred <- predict(m, pdat, type = "link", se.fit = TRUE)
ilink <- family(m)$linkinv # g-1()
pdat <- pdat %>%
  bind_cols(data.frame(pred)) %>%
  mutate(fitted = ilink(fit),
         upper = ilink(fit + (2 * se.fit)),
         lower = ilink(fit - (2 * se.fit)))
# plot
ggplot(wasp, aes(x = leafHeight,
                 y = as.numeric(visited))) +
    geom_point() +
    geom_ribbon(aes(ymin = lower, ymax = upper,
                    x = leafHeight), data = pdat,
                inherit.aes = FALSE, alpha = 0.2) +
    geom_line(data = pdat, aes(y = fitted)) +
    labs(x = "Leaf Height [cm]",
         y = "Probability of visitation")
```
]
]

.col-6[
```{r predict-darlingtonia, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 6}
```
]

]

---

# Wald statistics

$z$ values are Wald statistics, which under the null hypothesis are *asymptotically* standard normal

```{r coeftab-darlingtonia, results = "asis", echo = FALSE}
knitr::kable(round(summary(m)$coefficients, 4), format = "pipe")
```

<br />
Tests the null hypothesis that $\beta_i = 0$
$$z = \hat{\beta}_i / \mathrm{SE}(\hat{\beta}_i)$$

---

# Deviance

.small[
* In least squares we have the residual sum of squares as the measure of lack of fitted
* In GLMs, **deviance** plays the same role
* Deviance is defined as twice the log likelihood of the observed data under the current model
* Deviance is defined relative to an arbitrary constant &mdash; only **differences** of deviances have any meaning
* Differences in deviances are also known as ratios of likelihoods
* An alternative to the Wald tests are deviance ratio or likelihood ratio tests
    $$F = \frac{(D_a - D_b) / (\mathsf{df}_a - \mathsf{df}_b)}{D_b / \mathsf{df}_b}$$
* $D_j$ deviance of model, where we test if model A is a significant improvement over model B; $\mathsf{df}_k$ are the degrees of freedom of the respective model
]

---

# A Gamma GLM &mdash; simple age-depth modelling

Radiocarbon age estimates from depths within a peat bog (Brew & Maddy, 1995, QRA Technical Guide 5)

Estimate accumulation rate; assumption here is linear accumulation

Uncertainty or error is greater at depth; mean variance relationship

Fit mid-depth & mid-calibrated age points

---

# A Gamma GLM &mdash; simple age-depth modelling

```{r load-maddy, echo = TRUE}
maddy <- read_csv(here("data", "maddy-peat.csv"), col_types = "cdddddd")
maddy <- mutate(maddy, midDepth = upperDepth - (0.5 * abs(upperDepth - lowerDepth)),
                calMid = calUpper - (0.5 * abs(calUpper - calLower)))
maddy
```

---

# A Gamma GLM &mdash; simple age-depth modelling

.row[

.col-6[
```{r plot-maddy, echo = TRUE, eval = FALSE}
ggplot(maddy, aes(x = midDepth, y = calMid)) +
    geom_point() +
    labs(y = "Calibrated Age", x = "Depth")
```

]

.col-6[
```{r plot-maddy, echo = FALSE, eval = TRUE, fig.height = 6, fig.width = 6}
```
]
]

---

# A Gamma GLM

.smaller[
```{r peat-model, echo = TRUE}
m_gamma <- glm(calMid ~ midDepth, data = maddy, family = Gamma(link = "identity"))
summary(m_gamma)
```
]

---

# A Gamma GLM

.row[

.col-6[
.smaller[
```{r plot-maddy-fitted-gamma, eval=FALSE}
# data to predict at
pdat <- with(maddy,
             tibble(midDepth = seq(min(midDepth),
                                   max(midDepth),
                                   length = 100)))
# predict
p_gamma <- predict(m_gamma, pdat, type = "link",
                   se.fit = TRUE)
ilink <- family(m_gamma)$linkinv
# confidence interval
p_gamma <- pdat %>%
  bind_cols(data.frame(p_gamma)) %>%
  mutate(fitted = ilink(fit),
         upper = ilink(fit + (2 * se.fit)),
         lower = ilink(fit - (2 * se.fit)))
# plot
p1 <- ggplot(maddy, aes(x = midDepth, y = calMid)) +
    geom_ribbon(aes(ymin = lower, ymax = upper,
                    x = midDepth), data = p_gamma,
                inherit.aes = FALSE, alpha = 0.2) +
    geom_line(data = p_gamma, aes(y = fitted)) +
    geom_point() +
    labs(y = "Calibrated Age", x = "Depth",
         title = "Gamma GLM")
p1
```
]
]

.col-6[
```{r plot-maddy-fitted-gamma, echo = FALSE, eval = TRUE, fig.height = 6, fig.width = 6}
```

]
]

---

# A Gaussian GLM

.row[

.col-6[
.smaller[
```{r plot-maddy-fitted-gaussian, eval = FALSE}
# fit gaussian GLM
m_gaus <- glm(calMid ~ midDepth, data = maddy,
              family = gaussian)
# predict
p_gaus <- predict(m_gaus, pdat, type = "link",
                  se.fit = TRUE)
ilink <- family(m_gaus)$linkinv
# prep confidence interval
p_gaus <- pdat %>%
  bind_cols(data.frame(p_gaus)) %>%
  mutate(fitted = ilink(fit),
         upper = ilink(fit + (2 * se.fit)),
         lower = ilink(fit - (2 * se.fit)))
# plot
p2 <- ggplot(maddy, aes(x = midDepth, y = calMid)) +
    geom_ribbon(aes(ymin = lower, ymax = upper,
                    x = midDepth), data = p_gaus,
                inherit.aes = FALSE, alpha = 0.2) +
    geom_line(data = p_gaus, aes(y = fitted)) +
    geom_point() +
    labs(y = "Calibrated Age",
         x = "Depth",
         title = "Linear Model")
p2
```

]
]

.col-6[
```{r plot-maddy-fitted-gaussian, echo = FALSE, eval = TRUE, fig.height = 6, fig.width = 6}
```

]
]

---

```{r}
library("patchwork")
p1 + p2
```

---

# Transform or GLM?

```
m1 <- glm(log(y) ~ x, data = my_data, family = Gamma(link = "identity"))

m2 <- glm(y ~ x, data = my_data, family = Gamma(link = "log"))
```

These models are different

1. `m1` is a model for $\mathbb{E}(\log(y_i))$
2. `m2` is a model for $\mathbb{E}(y_i)$

$$
\log(\mathbb{E}(y_i)) \neq \mathbb{E}(\log(y_i))
$$

---
class: inverse middle center huge-subsection

# GAMs

---
class: inverse middle center subsection

# Motivating example

---

# HadCRUT4 time series

```{r hadcrut-temp-example, echo = FALSE}
URL <- "https://bit.ly/hadcrutv4"
# data are year, median of ensemble runs, certain quantiles in remaining cols
# take only cols 1 and 2
gtemp <- read_table(URL, col_types = 'nnnnnnnnnnnn', col_names = FALSE) %>%
    select(num_range('X', 1:2)) %>% setNames(nm = c('Year', 'Temperature'))

## Plot
gtemp_plt <- ggplot(gtemp, aes(x = Year, y = Temperature)) +
    geom_line() + 
    geom_point() +
    labs(x = 'Year', y = expression(Temeprature ~ degree*C))
gtemp_plt
```

???

Hadley Centre NH temperature record ensemble

How would you model the trend in these data?

---

# (Generalized) Linear Models

$$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$$

$$\mu_i = \beta_0 + \beta_1 \mathtt{year}_{i} + \beta_2 \mathtt{year}^2_{1i} + \cdots + \beta_j \mathtt{year}^j_{i}$$

Assumptions

1. linear effects of covariates are good approximation of the true effects
2. conditional on the values of covariates, $y_i | \mathbf{X} \sim \mathcal{N}(0, \sigma^2)$
3. this implies all observations have the same *variance*
4. $y_i | \mathbf{X}$ are *independent*

An **additive** model address the first of these

---
class: inverse center middle subsection

# Why bother with anything more complex?

---

# Is this linear?

```{r hadcrut-temp-example, echo = FALSE}
```

---

# Polynomials perhaps&hellip;

```{r hadcrut-temp-polynomial, echo = FALSE}
p <- c(1,3,8,15)
N <- 300
newd <- with(gtemp, data.frame(Year = seq(min(Year), max(Year), length = N)))
polyFun <- function(i, data = data) {
    lm(Temperature ~ poly(Year, degree = i), data = data)
}
mods <- lapply(p, polyFun, data = gtemp)
pred <- vapply(mods, predict, numeric(N), newdata = newd)
colnames(pred) <- p
newd <- cbind(newd, pred)
polyDat <- gather(newd, Degree, Fitted, - Year)
polyDat <- mutate(polyDat, Degree = ordered(Degree, levels = p))
gtemp_plt + geom_line(data = polyDat, mapping = aes(x = Year, y = Fitted, colour = Degree),
                      size = 1.5, alpha = 0.9) +
    scale_color_brewer(name = "Degree", palette = "PuOr") +
    theme(legend.position = "right")
```

---

# Polynomials perhaps&hellip;

We can keep on adding ever more powers of $\boldsymbol{x}$ to the model &mdash; model selection problem

**Runge phenomenon** &mdash; oscillations at the edges of an interval &mdash; means simply moving to higher-order polynomials doesn't always improve accuracy

---
class: inverse middle center subsection

# GAMs offer a solution

---

# HadCRUT data set

```{r read-hadcrut, echo = TRUE}
library('readr')
library('dplyr')
URL <-  "https://bit.ly/hadcrutv4"
gtemp <- read_table(URL, col_types = 'nnnnnnnnnnnn', col_names = FALSE) %>%
    select(num_range('X', 1:2)) %>% setNames(nm = c('Year', 'Temperature'))
```

[File format](https://www.metoffice.gov.uk/hadobs/hadcrut4/data/current/series_format.html)

---

# HadCRUT data set

```{r show-hadcrut, echo = TRUE, dependson = -1}
gtemp
```

---

# Fitting a GAM

```{r hadcrutemp-fitted-gam, echo = TRUE, results = 'hide'}
library('mgcv')
m <- gam(Temperature ~ s(Year), data = gtemp, method = 'REML')
summary(m)
```

.smaller[
```{r hadcrutemp-fitted-gam, echo = FALSE}
```
]

---

# Fitted GAM

```{r hadcrtemp-plot-gam, echo = FALSE}
N <- 300
newd <- as_tibble(with(gtemp, data.frame(Year = seq(min(Year), max(Year), length = N))))
pred <- as_tibble(as.data.frame(predict(m, newdata = newd, se.fit = TRUE,
                                        unconditional = TRUE)))
pred <- bind_cols(newd, pred) %>%
    mutate(upr = fit + 2 * se.fit, lwr = fit - 2*se.fit)

ggplot(gtemp, aes(x = Year, y = Temperature)) +
    geom_point() +
    geom_ribbon(data = pred,
                mapping = aes(ymin = lwr, ymax = upr, x = Year), alpha = 0.4, inherit.aes = FALSE,
                fill = "#fdb338") +
    geom_line(data = pred,
              mapping = aes(y = fit, x = Year), inherit.aes = FALSE, size = 1, colour = "#025196") +
    labs(x = 'Year', y = expression(Temeprature ~ degree*C))
```

---
class: inverse middle center big-subsection

# GAMs

---

# Generalized Additive Models

<br />

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---

# How is a GAM different?

$$\begin{align*}
y_i &\sim \mathcal{D}(\mu_i, \theta) \\ 
\mathbb{E}(y_i) &= \mu_i = g(\eta_i)^{-1}
\end{align*}$$

We model the mean of data as a sum of linear terms:

$$\eta_i = \beta_0 +\sum_j \color{red}{ \beta_j x_{ji}}$$

A GAM is a sum of _smooth functions_ or _smooths_

$$\eta_i = \beta_0 + \sum_j \color{red}{f_j(x_{ji})}$$

---

# How did `gam()` *know*?

```{r hadcrtemp-plot-gam, echo = FALSE}
```

---
class: inverse
background-image: url('./resources/rob-potter-398564.jpg')
background-size: contain

# What magic is this?

.footnote[
<a style="background-color:black;color:white;text-decoration:none;padding:4px 6px;font-family:-apple-system, BlinkMacSystemFont, &quot;San Francisco&quot;, &quot;Helvetica Neue&quot;, Helvetica, Ubuntu, Roboto, Noto, &quot;Segoe UI&quot;, Arial, sans-serif;font-size:12px;font-weight:bold;line-height:1.2;display:inline-block;border-radius:3px;" href="https://unsplash.com/@robpotter?utm_medium=referral&amp;utm_campaign=photographer-credit&amp;utm_content=creditBadge" target="_blank" rel="noopener noreferrer" title="Download free do whatever you want high-resolution photos from Rob Potter"><span style="display:inline-block;padding:2px 3px;"><svg xmlns="http://www.w3.org/2000/svg" style="height:12px;width:auto;position:relative;vertical-align:middle;top:-1px;fill:white;" viewBox="0 0 32 32"><title></title><path d="M20.8 18.1c0 2.7-2.2 4.8-4.8 4.8s-4.8-2.1-4.8-4.8c0-2.7 2.2-4.8 4.8-4.8 2.7.1 4.8 2.2 4.8 4.8zm11.2-7.4v14.9c0 2.3-1.9 4.3-4.3 4.3h-23.4c-2.4 0-4.3-1.9-4.3-4.3v-15c0-2.3 1.9-4.3 4.3-4.3h3.7l.8-2.3c.4-1.1 1.7-2 2.9-2h8.6c1.2 0 2.5.9 2.9 2l.8 2.4h3.7c2.4 0 4.3 1.9 4.3 4.3zm-8.6 7.5c0-4.1-3.3-7.5-7.5-7.5-4.1 0-7.5 3.4-7.5 7.5s3.3 7.5 7.5 7.5c4.2-.1 7.5-3.4 7.5-7.5z"></path></svg></span><span style="display:inline-block;padding:2px 3px;">Rob Potter</span></a>
]

---
class: inverse
background-image: url('resources/wiggly-things.png')
background-size: contain
