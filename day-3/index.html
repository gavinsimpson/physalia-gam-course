<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Generalized Additive Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gavin Simpson" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: inverse, middle, left, my-title-slide, title-slide

# Generalized Additive Models
## a data-driven approach to estimating regression models
### Gavin Simpson
### Department of Animal Science · Aarhus University
### 1400–2000 CET (1300–1900 UTC) Wednesday 16th, 2022

---

class: inverse middle center big-subsection



# Day 3

???

---

# Logistics

## Slides

Slidedeck: [bit.ly/physalia-gam-3](https://bit.ly/physalia-gam-3)

Sources: [bit.ly/physalia-gam](https://bit.ly/physalia-gam)

Direct download a ZIP of everything: [bit.ly/physalia-gam-zip](https://bit.ly/physalia-gam-zip)

Unpack the zip &amp; remember where you put it

---

# Matters arising

1. Basis functions for `s(x, z)`

2. Testing for non-linearity beyond a linear effect

3. Derivatives of smooths

---

# Basis functions for `s(x, z)`

Code for this is in `day-2/test.R`



.center[
&lt;img src="index_files/figure-html/tprs-2d-basis-1.svg" width="90%" /&gt;
]

---

# Basis functions for `s(x, z)`

Panels 1&amp;ndash;15 are the basis functions

.center[
&lt;img src="/home/au690221/work/teaching/physalia/gam-course/course/day-3/resources/wood-2ed-fig-5-12-2-d-tprs-basis-funs.png" width="45%" /&gt;
]

.small[
Source: Wood SN (2017)
]

---

# Beyond linearity

.row[

.col-6[

```r
n &lt;- 100
set.seed(2)
df &lt;- tibble(x = runif(n),
             y = x + x^2 * 0.2 + rnorm(n) * 0.1)

model &lt;- gam(y ~ x + s(x, m = c(2,0)),
             data = df, method = "REML")
```
]
.col-6[
.small[

```r
summary(model)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## y ~ x + s(x, m = c(2, 0))
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.02133    0.05315  -0.401    0.689    
## x            1.18249    0.10564  11.193   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##         edf Ref.df     F p-value  
## s(x) 0.9334      8 0.304   0.076 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =   0.91   Deviance explained = 91.1%
## -REML = -70.567  Scale est. = 0.012767  n = 100
```
]
]
]

---

# Beyond linearity


```r
draw(model, parametric = TRUE)
```

.center[
&lt;img src="index_files/figure-html/beyond-linearity-plot-1.svg" width="95%" /&gt;
]

---

# Derivatives of smooths

Use the `derivatives()` function in {gratia}

Provides

* First derivative
* Second derivative
* Finite differences
    * Forward
    * Backward
    * Central

If you want to estimate the the *n*th derivative, need to have a `\(n+1\)` derivative penalty

To estimate the second derivative you need to have `m = 3` say for TPRS

And set the finite difference `eps` to a much larger value than the default

---

# Derivatives of smooths

Simulated motorcycle collision data


```r
data(mcycle, package = "MASS")
m &lt;- gam(accel ~ s(times), data = mcycle, method = "REML")
sm_plt &lt;- draw(m, residuals = TRUE)
sm_plt
```

.center[
&lt;img src="index_files/figure-html/draw-mcycle-1.svg" width="90%" /&gt;
]

---

# Derivatives of `s(times)`


```r
fd &lt;- derivatives(m, type = "central", unconditional = TRUE)
fd
```

```
## # A tibble: 200 × 8
##    smooth   var    data derivative    se  crit lower upper
##    &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 s(times) times  2.40      -2.16  3.30  1.96 -8.62  4.30
##  2 s(times) times  2.68      -2.15  3.29  1.96 -8.60  4.31
##  3 s(times) times  2.95      -2.12  3.28  1.96 -8.55  4.31
##  4 s(times) times  3.23      -2.06  3.26  1.96 -8.45  4.32
##  5 s(times) times  3.51      -1.98  3.22  1.96 -8.29  4.33
##  6 s(times) times  3.79      -1.86  3.17  1.96 -8.08  4.35
##  7 s(times) times  4.06      -1.72  3.11  1.96 -7.81  4.37
##  8 s(times) times  4.34      -1.55  3.03  1.96 -7.48  4.38
##  9 s(times) times  4.62      -1.36  2.93  1.96 -7.10  4.39
## 10 s(times) times  4.90      -1.14  2.82  1.96 -6.67  4.40
## # … with 190 more rows
```

---

# Derivatives of `s(times)`


```r
fd_plt &lt;- draw(fd) + labs(title = "First derivative s(times)")
sm_plt + fd_plt + plot_layout(ncol = 2)
```

.center[
&lt;img src="index_files/figure-html/draw-derivatives-times-smooth-1.svg" width="90%" /&gt;
]

---

# Second derivative of `s(times)` &amp;mdash; wrong!


```r
fd2 &lt;- derivatives(m, order = 2, eps = 0.1, type = "central", unconditional = TRUE)
fd2_plt &lt;- draw(fd2)
fd_plt + fd2_plt + plot_layout(ncol = 2)
```

.center[
&lt;img src="index_files/figure-html/draw-derivatives-times-smooth-2-1.svg" width="80%" /&gt;
]

---

# Second derivative of `s(times)` &amp;mdash; right!


```r
m2 &lt;- gam(accel ~ s(times, m = 3), data = mcycle, method = "REML")
fd2 &lt;- derivatives(m2, order = 2, eps = 0.1, type = "central", unconditional = TRUE)
fd2_plt &lt;- draw(fd2) + labs(title = "Second derivative s(times)")
fd_plt + fd2_plt + plot_layout(ncol = 2)
```

.center[
&lt;img src="index_files/figure-html/draw-derivatives-times-smooth-2-right-1.svg" width="80%" /&gt;
]

---

# Today's topics

* Model checking, selection, and visualisation.

* How do we do inference with GAMs?

* Go beyond simple GAMs to include smooth interactions and models with multiples smooths.


---

# Smooth interactions

Two ways to fit smooth interactions

1. Bivariate (or higher order) thin plate splines
    * `s(x, z, bs = 'tp')`
    * Isotropic; single smoothness parameter for the smooth
	* Sensitive to scales of `x` and `z`
2. Tensor product smooths
    * Separate marginal basis for each smooth, separate smoothness parameters
	* Invariant to scales of `x` and `z`
	* Use for interactions when variables are in different units
	* `te(x, z)`

---



![](index_files/figure-html/draw-tprs-vs-tensor-product-truth-1.svg)&lt;!-- --&gt;

---

```{r-tprs-vs-tensor-product}
df
m_tprs &lt;- gam(y ~ s(x, z), data = df, method = "REML")
m_te   &lt;- gam(y ~ te(x, z), data = df, method = "REML")
```

---


```r
truth_plt + (draw(m_tprs) + coord_cartesian()) + draw(m_te) + plot_layout(ncol = 3)
```

![](index_files/figure-html/draw-tprs-vs-tensor-product-1.svg)&lt;!-- --&gt;

---


```r
layout(matrix(1:3, ncol = 3))
persp(xs, zs, truth)
vis.gam(m_tprs)
vis.gam(m_te)
layout(1)
```

![](index_files/figure-html/plot-tprs-vs-tensor-product-1.svg)&lt;!-- --&gt;

---

# Tensor product smooths

There are multiple ways to build tensor products in *mgcv*

1. `te(x, z)`
2. `t2(x, z)`
3. `s(x) + s(z) + ti(x, z)`

`te()` is the most general form but not usable in `gamm4::gamm4()` or *brms*

`t2()` is an alternative implementation that does work in `gamm4::gamm4()` or *brms*

`ti()` fits pure smooth interactions; where the main effects of `x` and `z` have been removed from the basis

---

# Tensor product smooths

.center[
&lt;img src="/home/au690221/work/teaching/physalia/gam-course/course/day-3/resources/wood-gams-2ed-fig-5-17-tensor-product.svg" width="50%" /&gt;
]

---

# Factor smooth interactions

Two ways for factor smooth interactions

1. `by` variable smooths
    * entirely separate smooth function for each level of the factor
	* each has it's own smoothness parameter
	* centred (no group means) so include factor as a fixed effect
	* `y ~ f + s(x, by = f)`
2. `bs = 'fs'` basis
    * smooth function for each level of the function
	* share a common smoothness parameter
	* fully penalized; include group means
	* closer to random effects
	* `y ~ s(x, f, bs = 'fs')`

---

# Random effects

When fitted with REML or ML, smooths can be viewed as just fancy random effects

Inverse is true too; random effects can be viewed as smooths

If you have simple random effects you can fit those in `gam()` and `bam()` without needing the more complex GAMM functions `gamm()` or `gamm4::gamm4()`

These two models are equivalent


```r
m_nlme &lt;- lme(travel ~ 1, data = Rail, ~ 1 | Rail, method = "REML") 

m_gam  &lt;- gam(travel ~ s(Rail, bs = "re"), data = Rail, method = "REML")
```

---

# Random effects

The random effect basis `bs = 're'` is not as computationally efficient as *nlme* or *lme4* for fitting

* complex random effects terms, or
* random effects with many levels

Instead see `gamm()` and `gamm4::gamm4()`

* `gamm()` fits using `lme()`
* `gamm4::gamm4()` fits using `lmer()` or `glmer()`

For non Gaussian models use `gamm4::gamm4()`

---
class: inverse center middle subsection

# Model checking

---

# Model checking

So you have a GAM:

- How do you know you have the right degrees of freedom? `gam.check()`

- Diagnosing model issues: `gam.check()` part 2

---

# GAMs are models too

How accurate your predictions will be depends on how good the model is

![](index_files/figure-html/misspecify-1.svg)&lt;!-- --&gt;

---
class: inverse center middle subsection

# How do we test how well our model fits?

---

# Simulated data


```r
set.seed(2)
n &lt;- 400
x1 &lt;- rnorm(n)
x2 &lt;- rnorm(n)
y_val &lt;- 1 + 2*cos(pi*x1) + 2/(1+exp(-5*(x2)))
y_norm &lt;- y_val + rnorm(n, 0, 0.5)
y_negbinom &lt;- rnbinom(n, mu = exp(y_val),size=10)
y_binom &lt;- rbinom(n,1,prob = exp(y_val)/(1+exp(y_val)))
```

---

# Simulated data

![](index_files/figure-html/sims_plot-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# gam.check() part 1: do you have the right functional form?

---

# How well does the model fit?

- Many choices: k, family, type of smoother, &amp;hellip;

- How do we assess how well our model fits?

---

# Basis size *k*

- Set `k` per term

- e.g. `s(x, k=10)` or `s(x, y, k=100)`

- Penalty removes "extra" wigglyness
    
	- *up to a point!*

- (But computation is slower with bigger `k`)

---

# Checking basis size


```r
norm_model_1 &lt;- gam(y_norm~s(x1, k = 4) + s(x2, k = 4), method = 'REML')
gam.check(norm_model_1)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 8 iterations.
## Gradient range [-0.0003467788,0.0005154578]
## (score 736.9402 &amp; scale 2.252304).
## Hessian positive definite, eigenvalue range [0.000346021,198.5041].
## Model rank =  7 / 7 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##         k'  edf k-index p-value    
## s(x1) 3.00 1.00    0.13  &lt;2e-16 ***
## s(x2) 3.00 2.91    1.04    0.83    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

# Checking basis size


```r
norm_model_2 &lt;- gam(y_norm ~ s(x1, k = 12) + s(x2, k = 4), method = 'REML')
gam.check(norm_model_2)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 11 iterations.
## Gradient range [-5.658609e-06,5.392657e-06]
## (score 345.3111 &amp; scale 0.2706205).
## Hessian positive definite, eigenvalue range [0.967727,198.6299].
## Model rank =  15 / 15 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##          k'   edf k-index p-value   
## s(x1) 11.00 10.84    0.99    0.38   
## s(x2)  3.00  2.98    0.86    0.01 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

# Checking basis size


```r
norm_model_3 &lt;- gam(y_norm ~ s(x1, k = 12) + s(x2, k = 12),method = 'REML')
gam.check(norm_model_3)
```

```
## 
## Method: REML   Optimizer: outer newton
## full convergence after 8 iterations.
## Gradient range [-1.136192e-08,6.794565e-13]
## (score 334.2084 &amp; scale 0.2485446).
## Hessian positive definite, eigenvalue range [2.812271,198.6868].
## Model rank =  23 / 23 
## 
## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
## indicate that k is too low, especially if edf is close to k'.
## 
##          k'   edf k-index p-value
## s(x1) 11.00 10.85    0.98    0.31
## s(x2) 11.00  7.95    0.95    0.15
```

---

# Checking basis size

![](index_files/figure-html/gam_check_norm4-1.svg)&lt;!-- --&gt;

---
class: inverse middle center subsection

# Model diagnostics

---
class: inverse middle center subsection

# Using gam.check() part 2: visual checks

---

# gam.check() plots

`gam.check()` creates 4 plots: 

1. Quantile-quantile plots of residuals. If the model is right, should follow 1-1 line

2. Histogram of residuals

3. Residuals vs. linear predictor

4. Observed vs. fitted values

`gam.check()` uses deviance residuals by default

---

# Gaussian data, Gaussian model


```r
norm_model &lt;- gam(y_norm ~ s(x1, k=12) + s(x2, k=12), method = 'REML')
gam.check(norm_model, rep = 500)
```

![](index_files/figure-html/gam_check_plots1-1.svg)&lt;!-- --&gt;

---

# Negative binomial data, Poisson model


```r
pois_model &lt;- gam(y_negbinom ~ s(x1, k=12) + s(x2, k=12), family=poisson, method= 'REML')
gam.check(pois_model, rep = 500)
```

![](index_files/figure-html/gam_check_plots2-1.svg)&lt;!-- --&gt;

---

# NB data, NB model


```r
negbin_model &lt;- gam(y_negbinom ~ s(x1, k=12) + s(x2, k=12), family = nb, method = 'REML')
gam.check(negbin_model, rep = 500)
```

![](index_files/figure-html/gam_check_plots3-1.svg)&lt;!-- --&gt;

---

# NB data, NB model


```r
appraise(negbin_model, method = 'simulate')
```

![](index_files/figure-html/appraise-gam-check-example-1.svg)&lt;!-- --&gt;

---
class: inverse center middle subsection

# Model selection

---

# Model selection

Model (or variable) selection &amp;mdash; an important area of theoretical and applied interest

- In statistics we aim for a balance between *fit* and *parsimony*

- In applied research we seek the set of covariates with strongest effects on `\(y\)`

We seek a subset of covariates that improves *interpretability* and *prediction accuracy*

---
class: inverse center middle

# Shrinkage &amp; additional penalties

---

# Shrinkage &amp; additional penalties

Smoothing parameter estimation allows selection of a wide range of potentially complex functions for smooths...

But, cannot remove a term entirely from the model because the penalties used act only on the *range space* of a spline basis. The *null space* of the basis is unpenalised.

- **Null space** &amp;mdash; the basis functions that are smooth (constant, linear)

- **Range space** &amp;mdash; the basis functions that are wiggly

---

# Shrinkage &amp; additional penalties

**mgcv** has two ways to penalize the null space, i.e. to do selection

- *double penalty approach* via `select = TRUE`

- *shrinkage approach* via special bases for
    
	- thin plate spline (default, `s(..., bs = 'ts')`),
    
	- cubic splines  (`s(..., bs = 'cs')`)

**double penalty** tends to works best, but applies to all smooths *and* doubles the number of smoothness parameters to estimate

Other shrinkage/selection approaches *are available* in other software

---

# Empirical Bayes...?

`\(\mathbf{S}_j\)` can be viewed as prior precision matrices and `\(\lambda_j\)` as improper Gaussian priors on the spline coefficients.

The impropriety derives from `\(\mathbf{S}_j\)` not being of full rank (zeroes in `\(\mathbf{\Lambda}_j\)`).

Both the double penalty and shrinkage smooths remove the impropriety from the Gaussian prior

---
# Empirical Bayes...?

- **Double penalty** &amp;mdash; makes no assumption as to how much to shrink the null space. This is determined from the data via estimation of `\(\lambda_j^{*}\)`

- **Shrinkage smooths** &amp;mdash; assumes null space should be shrunk less than the wiggly part

Marra &amp; Wood (2011) show that the double penalty and the shrinkage smooth approaches

- performed significantly better than alternatives in terms of *predictive ability*, and

- performed as well as alternatives in terms of variable selection

---

# Example

- Simulate Poisson counts
- 4 known functions (left)
- 2 spurious covariates (`runif()` &amp; not shown)


```r
## an example of automatic model selection via null space penalization
n &lt;- 200
dat &lt;- data_sim("eg1", n=n, scale=.15, dist = 'poisson', seed = 3) ## simulate data
dat &lt;- dat %&gt;% mutate(x4 = runif(n, 0, 1), x5 = runif(n, 0, 1),
                      f4 = rep(0, n), f5 = rep(0, n))   ## spurious
```

```r
b &lt;- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) +
             s(x4) + s(x5),
         data = dat, family = poisson, method = 'REML',
         select = TRUE)
```

---

# Example

![](index_files/figure-html/shrinkage-example-truth-1.svg)&lt;!-- --&gt;

---

# Example

.smaller[

```r
summary(b)
```

```
## 
## Family: poisson 
## Link function: log 
## 
## Formula:
## y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5)
## 
## Parametric coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.21758    0.04082   29.83   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##             edf Ref.df  Chi.sq p-value    
## s(x0) 1.7655088      9   5.264  0.0392 *  
## s(x1) 1.9271040      9  65.356  &lt;2e-16 ***
## s(x2) 6.1351414      9 156.204  &lt;2e-16 ***
## s(x3) 0.0002849      9   0.000  0.4181    
## s(x4) 0.0003044      9   0.000  0.9703    
## s(x5) 0.1756926      9   0.195  0.3018    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.545   Deviance explained = 51.6%
## -REML = 430.78  Scale est. = 1         n = 200
```
]

---

# Example


```r
draw(b, scales = 'fixed')
```

![](index_files/figure-html/shrinkage-example-plot-1.svg)&lt;!-- --&gt;

---
class: inverse center middle

# Credible intervals for smooths

---

# Credible intervals for smooths

`plot.gam()` produces approximate 95% intervals (at +/- 2 SEs)

What do these intervals represent?

Nychka (1988) showed that standard Wahba/Silverman type Bayesian confidence intervals on smooths had good **across-the-function** frequentist coverage properties

When *averaged* over the range of covariate, 1 - &amp;alpha; coverage is approximately 1 - &amp;alpha;

---

# Credible intervals for smooths

.center[
&lt;img src="/home/au690221/work/teaching/physalia/gam-course/course/day-3/resources/miller-bayesian-gam-interpretation-fig.svg" width="90%" /&gt;
]

---

# Credible intervals for smooths

Marra &amp; Wood (2012) extended this theory to the generalised case and explain where the coverage properties failed:

*Mustn't over-smooth too much, which happens when `\(\lambda_j\)` are over-estimated*

Two situations where this might occur

1. where true effect is almost in the penalty null space, `\(\hat{\lambda}_j \rightarrow \infty\)`
	- ie. close to a linear function
2. where `\(\hat{\lambda}_j\)` difficult to estimate due to highly correlated covariates
	- if 2 correlated covariates have different amounts of wiggliness, estimated effects can have degree of smoothness *reversed*

---

# Don't over-smooth

&gt; In summary, we have shown that Bayesian componentwise variable width intervals... for the smooth components of an additive model **should achieve close to nominal *across-the-function* coverage probability**&amp;hellip;

Basically

1. Don't over smooth, and

2. Effect of uncertainty due to estimating smoothness parameter is small

---

# Confidence intervals for smooths

Marra &amp; Wood (2012) suggested a solution to situation 1., namely true functions close to the penalty null space.

Smooths are normally subject to *identifiability* constraints (centred), which leads to zero variance where the estimated function crosses the zero line.

Instead, compute intervals for `\(j\)` th smooth as if it alone had the intercept; identifiability constraints go on the other smooth terms.

Use

* `seWithMean = TRUE` in call to `plot.gam()`
* `overall_uncertainty = TRUE` in call to `gratia::draw()`

---

# Example

![](index_files/figure-html/setup-confint-example-1.svg)&lt;!-- --&gt;

---
class: inverse center middle subsection

# *p* values for smooths

---

# Example

.smaller[

```r
summary(b)
```

```
## 
## Family: poisson 
## Link function: log 
## 
## Formula:
## y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5)
## 
## Parametric coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.21758    0.04082   29.83   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##             edf Ref.df  Chi.sq p-value    
## s(x0) 1.7655088      9   5.264  0.0392 *  
## s(x1) 1.9271040      9  65.356  &lt;2e-16 ***
## s(x2) 6.1351414      9 156.204  &lt;2e-16 ***
## s(x3) 0.0002849      9   0.000  0.4181    
## s(x4) 0.0003044      9   0.000  0.9703    
## s(x5) 0.1756926      9   0.195  0.3018    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =  0.545   Deviance explained = 51.6%
## -REML = 430.78  Scale est. = 1         n = 200
```
]

---

# *p* values for smooths

*p* values for smooths are approximate:

1. they don't account for the estimation of `\(\lambda_j\)` &amp;mdash; treated as known, hence *p* values are biased low

2. rely on asymptotic behaviour &amp;mdash; they tend towards being right as sample size tends to `\(\infty\)`

---

# *p* values for smooths

...are a test of **zero-effect** of a smooth term

Default *p* values rely on theory of Nychka (1988) and Marra &amp; Wood (2012) for confidence interval coverage

If the Bayesian CI have good across-the-function properties, Wood (2013a) showed that the *p* values have

- almost the correct null distribution

- reasonable power

Test statistic is a form of `\(\chi^2\)` statistic, but with complicated degrees of freedom

---

# *p* values for unpenalized smooths

The results of Nychka (1988) and Marra &amp; Wood (2012) break down if smooth terms are unpenalized

This include i.i.d. Gaussian random effects, (e.g. `bs = "re"`)

Wood (2013b) proposed instead a test based on a likelihood ratio statistic:

- the reference distribution used is appropriate for testing a `\(\mathrm{H}_0\)` on the boundary of the allowed parameter space...

- ...in other words, it corrects for a `\(\mathrm{H}_0\)` that a variance term is zero

---

# *p* values for smooths

Have the best behaviour when smoothness selection is done using **ML**, then **REML**.

Neither of these are the default, so remember to use `method = "ML"` or `method = "REML"` as appropriate

---

# AIC for GAMs

- Comparison of GAMs by a form of AIC is an alternative frequentist approach to model selection

- Rather than using the marginal likelihood, the likelihood of the `\(\mathbf{\beta}_j\)` *conditional* upon `\(\lambda_j\)` is used, with the EDF replacing `\(k\)`, the number of model parameters

- This *conditional* AIC tends to select complex models, especially those with random effects, as the EDF ignores that `\(\lambda_j\)` are estimated

- Wood et al (2016) suggests a correction that accounts for uncertainty in `\(\lambda_j\)`

`$$AIC = -2\mathcal{L}(\hat{\beta}) + 2\mathrm{tr}(\widehat{\mathcal{I}}V^{'}_{\beta})$$`

---

# AIC for GAMs


```r
b0 &lt;- gam(y ~ s(x0) + s(x1) + s(x2),
          data = dat, family = poisson, method = 'REML')
b1 &lt;- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5),
          data = dat, family = poisson, method = 'REML',
          select = TRUE)
b2 &lt;- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5),
          data = dat, family = poisson, method = 'REML')
```

---

# AIC

In this example, `\(x_3\)`, `\(x_4\)`, and `\(x_5\)` have no effects on `\(y\)`


```r
AIC(b0, b1, b2)
```

```
##          df      AIC
## b0 14.14062 841.5697
## b1 13.97258 840.5184
## b2 19.70168 845.8124
```

When there is *no difference* in compared models, accepts larger model ~16% of the time: consistent with probability AIC chooses a model with 1 extra spurious parameter `\(Pr(\chi^2_1 &gt; 2)\)`


```r
pchisq(2, 1, lower.tail = FALSE)
```

```
## [1] 0.1572992
```

---
class: inverse middle center subsection

# Example

---

# Galveston Bay

.row[

.col-6[
Cross Validated question

&gt; I have a dataset of water temperature measurements taken from a large waterbody at irregular intervals over a period of decades. (Galveston Bay, TX if you’re interested)

&lt;https://stats.stackexchange.com/q/244042/1390&gt;

]

.col-6[

.center[
![](/home/au690221/work/teaching/physalia/gam-course/course/day-3/resources/cross-validated.png)&lt;!-- --&gt;
]

]
]

---

# Galveston Bay


```r
galveston &lt;- read_csv(here('data', 'galveston.csv')) %&gt;%
    mutate(datetime = as.POSIXct(paste(DATE, TIME),
                                 format = '%m/%d/%y %H:%M', tz = "CDT"),
           STATION_ID = factor(STATION_ID),
           DoY = as.numeric(format(datetime, format = '%j')),
           ToD = as.numeric(format(datetime, format = '%H')) +
               (as.numeric(format(datetime, format = '%M')) / 60))
galveston
```

```
## # A tibble: 15,276 × 13
##    STATION_ID DATE     TIME   LATITUDE LONGITUDE  YEAR MONTH   DAY SEASON
##    &lt;fct&gt;      &lt;chr&gt;    &lt;time&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 
##  1 13296      6/20/91  11:04      29.5     -94.8  1991     6    20 Summer
##  2 13296      3/17/92  09:30      29.5     -94.8  1992     3    17 Spring
##  3 13296      9/23/91  11:24      29.5     -94.8  1991     9    23 Fall  
##  4 13296      9/23/91  11:24      29.5     -94.8  1991     9    23 Fall  
##  5 13296      6/20/91  11:04      29.5     -94.8  1991     6    20 Summer
##  6 13296      12/17/91 10:15      29.5     -94.8  1991    12    17 Winter
##  7 13296      6/29/92  11:17      29.5     -94.8  1992     6    29 Summer
##  8 13305      3/24/87  11:53      29.6     -95.0  1987     3    24 Spring
##  9 13305      4/2/87   13:39      29.6     -95.0  1987     4     2 Spring
## 10 13305      8/11/87  15:25      29.6     -95.0  1987     8    11 Summer
## # … with 15,266 more rows, and 4 more variables: MEASUREMENT &lt;dbl&gt;,
## #   datetime &lt;dttm&gt;, DoY &lt;dbl&gt;, ToD &lt;dbl&gt;
```

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

* `\(\alpha\)` is the model intercept,
* `\(f_1(\text{ToD}_i)\)` is a smooth function of time of day,
* `\(f_2(\text{DoY}_i)\)` is a smooth function of day of year ,
* `\(f_3(\text{Year}_i)\)` is a smooth function of year,
* `\(f_4(\text{x}_i, \text{y}_i)\)` is a 2D smooth of longitude and latitude,

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

* `\(f_5(\text{DoY}_i, \text{Year}_i)\)` is a tensor product smooth of day of year and year,
* `\(f_6(\text{x}_i, \text{y}_i, \text{ToD}_i)\)` tensor product smooth of location &amp; time of day
* `\(f_7(\text{x}_i, \text{y}_i, \text{DoY}_i)\)` tensor product smooth of location day of year&amp; 
* `\(f_8(\text{x}_i, \text{y}_i, \text{Year}_i\)` tensor product smooth of location &amp; year

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

Effectively, the first four smooths are the main effects of

1. time of day,
2. season,
3. long-term trend,
4. spatial variation

---

# Galveston Bay model description

$$
`\begin{align}
  \begin{split}
      \mathrm{E}(y_i) &amp; = \alpha + f_1(\text{ToD}_i) + f_2(\text{DoY}_i) + f_3(\text{Year}_i) + f_4(\text{x}_i, \text{y}_i) + \\
        &amp; \quad f_5(\text{DoY}_i, \text{Year}_i) + f_6(\text{x}_i, \text{y}_i, \text{ToD}_i) + \\
        &amp; \quad f_7(\text{x}_i, \text{y}_i, \text{DoY}_i) + f_8(\text{x}_i, \text{y}_i, \text{Year}_i)
  \end{split}
\end{align}`
$$

whilst the remaining tensor product smooths model smooth interactions between the stated covariates, which model

5. how the seasonal pattern of temperature varies over time,
6. how the time of day effect varies spatially,
7. how the seasonal effect varies spatially, and
8. how the long-term trend varies spatially

---

# Galveston Bay &amp;mdash; full model


```r
knots &lt;- list(DoY = c(0.5, 366.5))
m &lt;- bam(MEASUREMENT ~
             s(ToD, k = 10) +
             s(DoY, k = 12, bs = 'cc') +
             s(YEAR, k = 30) +
             s(LONGITUDE, LATITUDE, k = 100, bs = 'ds', m = c(1, 0.5)) +
             ti(DoY, YEAR, bs = c('cc', 'tp'), k = c(12, 15)) +
             ti(LONGITUDE, LATITUDE, ToD, d = c(2,1), bs = c('ds','tp'),
                m = list(c(1, 0.5), NA), k = c(20, 10)) +
             ti(LONGITUDE, LATITUDE, DoY, d = c(2,1), bs = c('ds','cc'),
                m = list(c(1, 0.5), NA), k = c(25, 12)) +
             ti(LONGITUDE, LATITUDE, YEAR, d = c(2,1), bs = c('ds','tp'),
                m = list(c(1, 0.5), NA), k = c(25, 15)),
         data = galveston, method = 'fREML', knots = knots,
         nthreads = c(4, 1), discrete = TRUE)
```

---

# Galveston Bay &amp;mdash; simpler model


```r
m.sub &lt;- bam(MEASUREMENT ~
             s(ToD, k = 10) +
             s(DoY, k = 12, bs = 'cc') +
             s(YEAR, k = 30) +
             s(LONGITUDE, LATITUDE, k = 100, bs = 'ds', m = c(1, 0.5)) +
             ti(DoY, YEAR, bs = c('cc', 'tp'), k = c(12, 15)),
         data = galveston, method = 'fREML', knots = knots,
         nthreads = c(4, 1), discrete = TRUE)
```

---

# Galveston Bay &amp;mdash; simpler model?


```r
AIC(m, m.sub)
```

```
##             df      AIC
## m     442.5221 58590.61
## m.sub 238.2766 59193.50
```
---

# Galveston Bay &amp;mdash; simpler model?

.smaller[

```r
anova(m, m.sub, test = 'F')
```

```
## Analysis of Deviance Table
## 
## Model 1: MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)) + 
##     ti(LONGITUDE, LATITUDE, ToD, d = c(2, 1), bs = c("ds", "tp"), 
##         m = list(c(1, 0.5), NA), k = c(20, 10)) + ti(LONGITUDE, 
##     LATITUDE, DoY, d = c(2, 1), bs = c("ds", "cc"), m = list(c(1, 
##         0.5), NA), k = c(25, 12)) + ti(LONGITUDE, LATITUDE, YEAR, 
##     d = c(2, 1), bs = c("ds", "tp"), m = list(c(1, 0.5), NA), 
##     k = c(25, 15))
## Model 2: MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15))
##   Resid. Df Resid. Dev      Df Deviance      F    Pr(&gt;F)    
## 1     14744      39093                                      
## 2     15022      41769 -278.27  -2675.8 3.6508 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]

---

# Galveston Bay &amp;mdash; full model summary

.small[

```r
summary(m)
```

```
## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## MEASUREMENT ~ s(ToD, k = 10) + s(DoY, k = 12, bs = "cc") + s(YEAR, 
##     k = 30) + s(LONGITUDE, LATITUDE, k = 100, bs = "ds", m = c(1, 
##     0.5)) + ti(DoY, YEAR, bs = c("cc", "tp"), k = c(12, 15)) + 
##     ti(LONGITUDE, LATITUDE, ToD, d = c(2, 1), bs = c("ds", "tp"), 
##         m = list(c(1, 0.5), NA), k = c(20, 10)) + ti(LONGITUDE, 
##     LATITUDE, DoY, d = c(2, 1), bs = c("ds", "cc"), m = list(c(1, 
##         0.5), NA), k = c(25, 12)) + ti(LONGITUDE, LATITUDE, YEAR, 
##     d = c(2, 1), bs = c("ds", "tp"), m = list(c(1, 0.5), NA), 
##     k = c(25, 15))
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 21.83529    0.08755   249.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Approximate significance of smooth terms:
##                                 edf Ref.df        F  p-value    
## s(ToD)                        3.357   4.05    5.626 0.000147 ***
## s(DoY)                        9.551  10.00 3266.413  &lt; 2e-16 ***
## s(YEAR)                      27.987  28.74   54.565  &lt; 2e-16 ***
## s(LONGITUDE,LATITUDE)        54.264  99.00    5.177  &lt; 2e-16 ***
## ti(DoY,YEAR)                131.329 140.00   34.598  &lt; 2e-16 ***
## ti(ToD,LONGITUDE,LATITUDE)   42.235 171.00    0.872  &lt; 2e-16 ***
## ti(DoY,LONGITUDE,LATITUDE)   80.831 240.00    1.189  &lt; 2e-16 ***
## ti(YEAR,LONGITUDE,LATITUDE)  83.543 329.00    1.080  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-sq.(adj) =   0.94   Deviance explained = 94.2%
## fREML =  29810  Scale est. = 2.6339    n = 15276
```
]

---

# Galveston Bay &amp;mdash; full model plot


```r
plot(m, pages = 1, scheme = 2, shade = TRUE)
```

![](index_files/figure-html/galveston-full-model-plot-1.svg)&lt;!-- --&gt;

---

# Galveston Bay &amp;mdash; full model plot


```r
draw(m, scales = 'free')
```

![](index_files/figure-html/galveston-full-model-draw-1.svg)&lt;!-- --&gt;

---

# Galveston Bay &amp;mdash; predict


```r
pdata &lt;- with(galveston,
              expand.grid(ToD = 12,
                          DoY = 180,
                          YEAR = seq(min(YEAR), max(YEAR), by = 1),
                          LONGITUDE = seq_min_max(LONGITUDE, n = 100),
                          LATITUDE  = seq_min_max(LATITUDE, n = 100)))
fit &lt;- predict(m, pdata)
ind &lt;- exclude.too.far(pdata$LONGITUDE, pdata$LATITUDE,
                       galveston$LONGITUDE, galveston$LATITUDE, dist = 0.1)
fit[ind] &lt;- NA
pred &lt;- cbind(pdata, Fitted = fit)
```

---

# Galveston Bay &amp;mdash; plot


```r
plt &lt;- ggplot(pred, aes(x = LONGITUDE, y = LATITUDE)) +
    geom_raster(aes(fill = Fitted)) + facet_wrap(~ YEAR, ncol = 12) +
    scale_fill_viridis(name = expression(degree*C), option = 'plasma', na.value = 'transparent') +
    coord_quickmap() +
    theme(legend.position = 'right')
plt
```

---

# Galveston Bay &amp;mdash; plot

![](index_files/figure-html/galveston-full-predict-plot-1.svg)&lt;!-- --&gt;

---

# Galveston Bay &amp;mdash;



.center[![](resources/galveston-animation.gif)]

---

# Galveston Bay &amp;mdash; plot trends


```r
pdata &lt;- with(galveston,
              expand.grid(ToD = 12,
                          DoY = c(1, 90, 180, 270),
                          YEAR = seq(min(YEAR), max(YEAR), length = 500),
                          LONGITUDE = -94.8751,
                          LATITUDE  = 29.50866))

fit &lt;- data.frame(predict(m, newdata = pdata, se.fit = TRUE))
fit &lt;- transform(fit, upper = fit + (2 * se.fit), lower = fit - (2 * se.fit))
pred &lt;- cbind(pdata, fit)

plt2 &lt;- ggplot(pred, aes(x = YEAR, y = fit, group = factor(DoY))) +
    geom_ribbon(aes(ymin = lower, ymax = upper), fill = 'grey', alpha = 0.5) +
    geom_line() + facet_wrap(~ DoY, scales = 'free_y') +
    labs(x = NULL, y = expression(Temperature ~ (degree * C)))
plt2
```

---

# Galveston Bay &amp;mdash; plot trends

![](index_files/figure-html/galveston-trends-by-month-1.svg)&lt;!-- --&gt;

---

# Next steps

Read Simon Wood's book!

Lots more material on our ESA GAM Workshop site

[https://noamross.github.io/mgcv-esa-workshop/]()

Noam Ross' free GAM Course

&lt;https://noamross.github.io/gams-in-r-course/&gt;

Noam also maintains a list of [GAM Resources](https://github.com/noamross/gam-resources)

A couple of papers:

.smaller[
1. Simpson, G.L., 2018. Modelling Palaeoecological Time Series Using Generalised Additive Models. Frontiers in Ecology and Evolution 6, 149. https://doi.org/10.3389/fevo.2018.00149
2. Pedersen, E.J., Miller, D.L., Simpson, G.L., Ross, N., 2019. Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ 7, e6876. https://doi.org/10.7717/peerj.6876
]

Also see my blog: [fromthebottomoftheheap.net](http://fromthebottomoftheheap.net)

---

# Reuse

* HTML Slide deck [bit.ly/physalia-gam-3](https://bit.ly/physalia-gam-3) &amp;copy; Simpson (2020) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [Source](https://bit.ly/physalia-gam)

---

# References

- [Marra &amp; Wood (2011) *Computational Statistics and Data Analysis* **55** 2372&amp;ndash;2387.](http://doi.org/10.1016/j.csda.2011.02.004)
- [Marra &amp; Wood (2012) *Scandinavian Journal of Statistics, Theory and Applications* **39**(1), 53&amp;ndash;74.](http://doi.org/10.1111/j.1467-9469.2011.00760.x.)
- [Nychka (1988) *Journal of the American Statistical Association* **83**(404) 1134&amp;ndash;1143.](http://doi.org/10.1080/01621459.1988.10478711)
- Wood (2017) *Generalized Additive Models: An Introduction with R*. Chapman and Hall/CRC. (2nd Edition)
- [Wood (2013a) *Biometrika* **100**(1) 221&amp;ndash;228.](http://doi.org/10.1093/biomet/ass048)
- [Wood (2013b) *Biometrika* **100**(4) 1005&amp;ndash;1010.](http://doi.org/10.1093/biomet/ast038)
- [Wood et al (2016) *JASA* **111** 1548&amp;ndash;1563](https://doi.org/10.1080/01621459.2016.1180986)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
