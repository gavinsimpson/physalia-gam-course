---
title: "Generalized Additive Models"
subtitle: "a data-driven approach to estimating regression models"
institute: "Department of Animal & Veterinary Sciences Â· Aarhus University"
author: "Gavin Simpson"
date: "1400&ndash;1900 CET (1300&ndash;1800 UTC) Friday 24th January, 2025"
output:
  xaringan::moon_reader:
    css: ["default", "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css", "slides.css"]
    lib_dir: libs
    nature:
      titleSlideClass: ["inverse","middle","left",my-title-slide]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
      ratio: "16:9"
---
class: inverse middle center big-subsection

```{r setup, include=FALSE, cache=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = FALSE, dev = "svg", echo = TRUE, message = FALSE,
                      warning = FALSE,
                      fig.height = 6, fig.width = 1.777777 * 6)

library("gridGraphics")
library("here")
library("mgcv")
library("qgam")
library("gratia")
library("ggplot2")
library("forcats")
library("purrr")
library("mvnfast")
library("tibble")
library("patchwork")
library("tidyr")
library("knitr")
library("viridis")
library("readr")
library("dplyr")
library("sf")

## plot defaults
theme_set(theme_bw(base_size = 16, base_family = "Fira Sans"))

```

```{r xaringan-tile-view, echo = FALSE, eval = TRUE}
# If you don't have xaringanExtra or can't get it installed, just change
#   eval = TRUE above to eval = FALSE, you don't need it to use the slidedeck
#
# To install it, you ned the remotes pkg installed then run the next line
#   to install xaringanExtra from GitHUb
#
# remotes::install_github("gadenbuie/xaringanExtra")
#
# then this code chunk will work
xaringanExtra::use_tile_view()
```

# Day 5

???

---

# Logistics

## Slides

Slidedeck: [bit.ly/physalia-gam-5](https://bit.ly/physalia-gam-5)

Sources: [bit.ly/physalia-gam](https://bit.ly/physalia-gam)

Direct download a ZIP of everything: [bit.ly/physalia-gam-zip](https://bit.ly/physalia-gam-zip)

Unpack the zip & remember where you put it

---
class: inverse center middle subsection

# Time series

---

# Smoothing autocorrelated data

Smoothing temporally autocorrelated data can `-->` over fitting

.row[

.col-6[

$y$ is contaminated with AR(1) noise

.smaller[
```{r correlated-data-eg, fig.show = "hide"}
set.seed(321)
n <- 100
time <- 1:n
xt <- time/n
Y <- (1280 * xt^4) * (1- xt)^4
y <- as.numeric(Y + arima.sim(list(ar = 0.3713),
                              n = n))
df <- tibble(y = y, time = time, f = Y)

# plot
plt <- ggplot(df, aes(x = time, y = y)) +
  geom_point() +
  geom_line(aes(y = f),
            col = "steelblue", lwd = 2)
plt
```
]
]

.col-6[
```{r correlated-data-eg, echo = FALSE, fig.width = 6, fig.height = 6}
```
]
]

---

# Smoothing autocorrelated data

.row[

.col-6[
.smaller[
```{r fit-correlated-data-eg, fig.show = "hide"}
# standard fit
m_reml <- gam(y ~ s(time, k = 20), data = df,
              method = "REML")
# use GCV
m_gcv <- gam(y ~ s(time, k = 20), data = df)

# fitted values
fv_reml <- fitted_values(m_reml)
fv_gcv <- fitted_values(m_gcv)

# plot
plt + geom_line(data = fv_reml,
               aes(x = time, y = .fitted),
               col = "red") +
  geom_line(data = fv_gcv,
            aes(x = time, y = .fitted),
            col = "darkgreen")
```
]
]

.col-6[

```{r fit-correlated-data-eg, echo = FALSE, fig.width = 6, fig.height = 6}
```
]
]

---

# Smoothing autocorrelated data

What about smoothing where $x$ is time? Is this still a problem?

--

Yes *and* No

--

Depends on how you want to decompose *time* again

---

# Temporal dependence

Temporal dependence really means that observations that are close together in time tend to be similar to one another than observations well separated in time

How similar depends on the memory length of the system

--

Strong dependence &mdash; high autocorrelatation &,dash; long memory

Weak dependence &mdash; low autocorrelatation &,dash; short memory

---

# Temporal dependence

What does a GAM say about our data?

--

It says that observations near to one another (in covariate space) are more similar than observations further apart

---

# Temporal dependence & GAMs

From this perspective then

Wiggly smooths = Strong dependence

Smooth (!) smooths = Weak dependence

---

# Temporal dependence & GAMs

If you don't like your trend to be that wiggly, what to do?

--

You could decompose the temporal effect into a smooth trend *plus* an autocorrelated process in the $\varepsilon$

--

That process could be ARMA(*p*, *q*) or a continuous time AR(1)

--

Fit with `gamm()` using `correlation` argument

---

# Smoothing autocorrelated data

.row[

.col-6[
.smaller[
```{r fit-correlated-data-gamm, fig.show = "hide"}
# standard fit
m_ar1 <- gamm(y ~ s(time, k = 20), data = df,
              correlation = corAR1(form = ~ 1),
              method = "REML")

# fitted values
fv_ar1 <- fitted_values(m_ar1$gam)

# plot
plt +
  geom_ribbon(data = fv_ar1,
              aes(ymin = .lower_ci, ymax = .upper_ci,
                  y = NULL),
              alpha = 0.2, fill = "hotpink") +
  geom_line(data = fv_ar1,
            aes(x = time, y = .fitted),
            col = "hotpink", lwd = 1.5)
```
]
]

.col-6[

```{r fit-correlated-data-gamm, echo = FALSE, fig.width = 6, fig.height = 6}
```
]
]

---

# But&hellip;

This can only work if the trend and the autocorrelation process are separately identifiable from the data

Or you are willing to impose constraints on one of

* the smooth of time (using a low `k`), or

* specify the parameters of the autocorrelation process

See [Simpson (2018)](https://doi.org/10.3389/fevo.2018.00149) for a brief discussion on this plus examples & the cited references therein

---
class: inverse center middle subsection

# Example

---
class: inverse center middle subsection

# Distributional models

---

# Distributional models

So far we have modelled the mean or $\mathbb{E}(\boldsymbol{y})$

We either assumed the variance was constant (Gaussian) or followed a prescribed form implied by the family / random component used (GLMs / GAMs)

What if the data don't follow these assumptions?

--

We could model all the parameters of the distribution

---

# Parameters beyond the mean

```{r gaussian-distributions-plt, echo = FALSE}
x <- seq(8, -8, length = 500)
df <- data.frame(density = c(dnorm(x, 0, 1), dnorm(x, 0, 2), dnorm(x, 2, 1), dnorm(x, -2, 1)),
                 x = rep(x, 4),
                 distribution = factor(rep(c("mean = 0; var = 1", "mean = 0; var = 4",
                                             "mean = 2; var = 1", "mean = -2; var = 1"), each = 500),
                                       levels = c("mean = 0; var = 1", "mean = 0; var = 4",
                                                  "mean = 2; var = 1", "mean = -2; var = 1")))
plt1 <- ggplot(subset(df, distribution %in% c("mean = 0; var = 1", "mean = 0; var = 4")),
               aes(x = x, y = density, colour = distribution)) +
    geom_line(size = 1) + theme(legend.position = "top") +
    guides(col = guide_legend(title = "Distribution", nrow = 2, title.position = "left")) +
    labs(x = "x", y = "Probability density")

plt2 <- ggplot(subset(df, distribution %in% c("mean = 2; var = 1", "mean = -2; var = 1")),
               aes(x = x, y = density, colour = distribution)) +
    geom_line(size = 1) + theme(legend.position = "top") +
    guides(col = guide_legend(title = "Distribution", nrow = 2, title.position = "left")) +
    labs(x = "x", y = "Probability density")

plt <- plt1 + plt2
plt
```

???

To do this we'll need models for the variance of a data set

If we think of the Gaussian distribution that distribution has two parameters, the mean and the variance

In linear regression we model the mean of the response at different values of the covariates, and assume the variance is constant at a value estimated from the residuals

In the left panel I'm showing how the Gaussian distribution changes as we alter the mean while keeping the variance fixed, while in the right panel I keep the mean fixed but vary the variance &mdash; the parameters are independent

---

# Distributional models

.medium[
$$y_{i} | \boldsymbol{x}_i \sim \mathcal{D}(\vartheta_{1}(\boldsymbol{x}_i), \ldots, \vartheta_{K}(\boldsymbol{x}_i))$$
]

For the Gaussian distribution

* $\vartheta_{1}(\boldsymbol{x}_i) = \mu(\boldsymbol{x}_i)$

* $\vartheta_{2}(\boldsymbol{x}_i) = \sigma(\boldsymbol{x}_i)$

???

Instead of treating the variance as a nuisance parameter, we could model both the variance **and** the mean as functions of the covariates

This is done using what is called a *distributional model*

In this model we say that the response values y_i, given the values of one or more covariates x_i follow some distribution with parameters theta, which are themselves functions of one or more covariates

For the Gaussian distribution theta 1 would be the mean and theta 2 the variance (or standard deviation)

We do not need to restrict ourselves to the Gaussian distribution however

---

# Pseudonyms

These models in GAM form were originally termed GAMLSS

GAMs for *L*ocation *S*cale *S*hape ([Rigby & Stasinopoulos, 2005](http://doi.org/10.1111/j.1467-9876.2005.00510.x)) in the {gamlss} ðŸ“¦

But the name *Distributional* model is more general

---

# Distributional models

In {mgcv} ðŸ“¦ special `family` functions are provided for some distributions which allow all parameters of the distribution to be modelled


.row[
.col-5[
* `gaulss()` Gaussian
* `ziplss()` ZI Poisson
* `twlss()` Tweedie
* `gevlss()` GEV
]
.col-7[
* `gamals()` Gamma
* `shash()` 4 parameter Sinh-arcsinh
* `gumbls()` Gumble
]
]

---

# Distributional models

Provide a list of formulas, 1 per linear predictor

```{r eval = FALSE}
gam(list(accel ~ s(times, k = 20, bs = "ad"),
               ~ s(times, k = 10)),
         data = mcycle,
         method = "REML", # <== IIRC REML is only option for these LSS
         family = gaulss())
```

And you need to really understand how these models are parameterised internally and what they are actually fitting and returning

--

**Read the relevant manual page**

---
class: inverse center middle subsection

# Example


---
class: inverse center middle subsection

# Quantile GAMs

---

# Quantile GAMs

Briefly, estimating the entire conditional distribution of $\mathbf{y}$ may not be necessary

--

Or even wasteful

--

Instead, if our focus is on a particular part (**quantile**) of the distribution may be better to try to estimate that instead of the whole distribution

--

Quantile regression methods do this

---

# Quantile GAMs

Smoothers for quantile regression models have been available for a long time in the {quantreg} ðŸ“¦

These models are estimated using the *pinball loss*

This is suboptimal for model fitting especially with smoothness selection and the ideas implemented in {mgcv} ðŸ“¦

Instead, [Fasiolo *et al* (2020)](https://doi.org/10.1080/01621459.2020.1725521) ([arXiv version](https://arxiv.org/abs/1707.03307)) developed a well-calibrated way of estimating QGAMs with smoothness selection, good inference properties, etc, using a Bayesian belief updating technique

Implemented in the {qgam} ðŸ“¦ that builds upon {mgcv} so stuff just works

---

# Quantile GAMs

```{r qgam-example, results = "hide"}
## Load the mcycle data
data(mcycle, package = "MASS")

## Fit QGAM using an adaptive smoother
m_q <- qgam(accel ~ s(times, k = 20, bs = "ad"),
            data = mcycle, # <== no family either
            qu = 0.8) #<<
```

```{r qgam-example-plt, echo = FALSE, out.width = "70%", fig.align = "center"}
## predict to visualise the estimated quantile
## new data to predict at
new_df <- with(mcycle,
               tibble(times = seq(round(min(times)), round(max(times)),
                                  length.out = 200)))
pred <- predict(m_q, newdata = new_df, se.fit = TRUE) %>%
  as.data.frame() %>%
  as_tibble() %>%
  setNames(c(".estimate", ".se")) %>%
  add_confint() %>%
  bind_cols(new_df)

## plot
pred %>%
  ggplot(aes(x = times)) +
    geom_point(data = mcycle, aes(x = times, y = accel)) +
    geom_ribbon(aes(ymin = .lower_ci, ymax = .upper_ci), alpha = 0.2) +
    geom_line(aes(y = .estimate))
```

---
class: inverse center middle subsection

# Example

---
class: inverse center middle subsection

# Bayesian GAMs

---

# Bayesian GAMs

Briefly (see [Miller 2021](http://arxiv.org/abs/1902.01330) & Wood 2017 for the gory details)&hellip;

$$\mathcal{L}_{p}(\boldsymbol{\beta}, \boldsymbol{\lambda}) = \mathcal{L}(\boldsymbol{\beta})\exp(-\boldsymbol{\beta}^{\mathsf{T}}\boldsymbol{S}_{\boldsymbol{\lambda}}\boldsymbol{\beta})$$

The exponential term is proportional to a mean zero MVN with prior precision matrix $\boldsymbol{S}_{\boldsymbol{\lambda}}$

This implies

$$\boldsymbol{\beta} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{S}_{\boldsymbol{\lambda}}^{-})$$

where $\boldsymbol{S}_{\boldsymbol{\lambda}}^{-}$ is a (pseudo)inverse of $\boldsymbol{S}_{\boldsymbol{\lambda}}$

---

# Baysian GAMs

Large entries in $\boldsymbol{S}$ correspond to more wiggly basis functions

We want to penalize these wiggly functions more strongly as we think smoother functions are more likely than wiggly functions

When we invert $\boldsymbol{S}$, these large values turn into small variances

Our prior is that that basis function's coefficient is close to zero

---

# Bayesian GAMs

REML and ML methods are *empirical Bayes* methods

Treating smooths as random effects implies the (improper) Gaussian priors on $\boldsymbol{\beta}$

We fit by maximising the likelihood given this implied prior

*empirical* because we do not have priors for the $\boldsymbol{\lambda}$

---

# Priors

It can be very hard to come up with good priors for smoothing parameters themselves

* the true value may be $\infty$ for a linear effect
* making an informed decision (guess) is hard because the $\boldsymbol{\lambda}$ are not directly interpretable in terms of something a user is familiar with

Fully Bayesian approach would use

* a vague gamma prior on the each element of $\boldsymbol{\lambda}$, or
* uniform priors on each element of $\log(\boldsymbol{\lambda})$

---

# Obtaining posteriors

In the fully Bayesian approach we attach priors to the $\boldsymbol{\lambda}$ and $\boldsymbol{\beta}$

Then use MCMC to obtain posterior draws for all the parameters and the response data

* `mgcv:::jagam()` writes out a *JAGS* code/model file for a specified GAM, edit it, then do the sampling in `JAGS`
* `brms::brm()` takes a formula using {mgcv} notation (`t2()` & `s()` only) for smooths and fits the model using `Stan`
* `BayesX` is a specialised software for fitting Bayesian GAMs. R interface via `BayesX` ðŸ“¦
* `bamlss::bamlss()` fits GAMs with {mgcv} notation & smooths via a range of *engines* (`JAGS`, `BayesX`,  internal code, &hellip;)

---

# Obtaining posteriors

In the empirical Bayes world we set priors (implied or otherwise) on the $\boldsymbol{\beta}$ but not the $\boldsymbol{\lambda}$ &mdash; posterior for $\boldsymbol{\beta}$ is conditional on $\boldsymbol{\lambda}$

---
class: inverse center middle subsection

# Example

---
class: inverse center middle subsection

# Overview

---

# Overview

* We choose to use GAMs when we expect non-linear relationships between covariates and $y$

* GAMs represent non-linear functions $fj(x_{ij})$ using splines

* Splines are big functions made up of little functions &mdash; *basis function*

* Estimate a coefficient $\beta_k$ for each basis function $b_k$

* As a user we need to set `k` the upper limit on the wiggliness for each $f_j()$

* Avoid overfitting through a wiggliness penalty &mdash; curvature or 2nd derivative

---

# Overview

* GAMs are just fancy GLMs &mdash; usual diagnostics apply `gam.check()` or `appraise()`

* Check you have the right distribution `family` using QQ plot, plot of residuls vs $\eta_i$, DHARMa residuals

* But have to check that the value(s) of `k` were large enough with `k.check()`

* Model selection can be done with `select = TRUE` or `bs = "ts"` or `bs = "cs"`

* Plot your fitted smooths using `plot.gam()` or `draw()`

* Produce hypotheticals using `data_slice()` and `fitted_values()` or `predict()`

---

# Overview

* Avoid fitting multiple models dropping terms in turn

* Can use AIC to select among mondels for prediction

* GAMs should be fitted with `method = "REML"` or `"ML"`

* Then they are an empirical Bayesian model

* Can explore uncertainty in estimates by smapling from the posterior of smooths or the model

---

# Overview

* The default basis is the low-rank thin plate spline

* Good properties but can be slow to set up &mdash; use `bs = "cr"` with big data

* Other basis types are available &mdash; most aren't needed in general but do have specific uses

* Tensor product smooths allow us to add smooth interactions to our models with `te()` or `t2()`

* `s()` can be used for multivariate smooths, but assumes isotropy

* Use `s(x) + s(z) + ti(x,z)` to test for an interaction

---

# Overview

* Smoothing temporal or spatial data can be tricky due to autocorrelation

* In some cases we can fit separate smooth trends & autocorrelatation processes

* But they can fail often

* Including smooths of space and time in your model can remove other effects: **confounding**

---

# Overview

* {mgcv} smooths can be used in other software

* Bayesian GAMs well catered for with {brms}

* Consider more than the mean parameter &mdash; distributional GAMs

* Consider modeling empirical quantiles using quantile GAMs

---

# Next steps

Read Simon Wood's book!

Lots more material on our ESA GAM Workshop site

[https://noamross.github.io/mgcv-esa-workshop/]()

Noam Ross' free GAM Course <https://noamross.github.io/gams-in-r-course/>

Noam also maintains a list of [GAM Resources](https://github.com/noamross/gam-resources)

A couple of papers:

.smaller[
1. Simpson, G.L., 2018. Modelling Palaeoecological Time Series Using Generalised Additive Models. Frontiers in Ecology and Evolution 6, 149. https://doi.org/10.3389/fevo.2018.00149
2. Pedersen, E.J., Miller, D.L., Simpson, G.L., Ross, N., 2019. Hierarchical generalized additive models in ecology: an introduction with mgcv. PeerJ 7, e6876. https://doi.org/10.7717/peerj.6876
]

Also see my blog: [fromthebottomoftheheap.net](http://fromthebottomoftheheap.net)

---

# Reuse

* HTML Slide deck [bit.ly/physalia-gam-5](https://bit.ly/physalia-gam-5) &copy; Simpson (2020-2022) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
* RMarkdown [Source](https://bit.ly/physalia-gam)

---

# References

- [Marra & Wood (2011) *Computational Statistics and Data Analysis* **55** 2372&ndash;2387.](http://doi.org/10.1016/j.csda.2011.02.004)
- [Marra & Wood (2012) *Scandinavian Journal of Statistics, Theory and Applications* **39**(1), 53&ndash;74.](http://doi.org/10.1111/j.1467-9469.2011.00760.x.)
- [Nychka (1988) *Journal of the American Statistical Association* **83**(404) 1134&ndash;1143.](http://doi.org/10.1080/01621459.1988.10478711)
- Wood (2017) *Generalized Additive Models: An Introduction with R*. Chapman and Hall/CRC. (2nd Edition)
- [Wood (2013a) *Biometrika* **100**(1) 221&ndash;228.](http://doi.org/10.1093/biomet/ass048)
- [Wood (2013b) *Biometrika* **100**(4) 1005&ndash;1010.](http://doi.org/10.1093/biomet/ast038)
- [Wood et al (2016) *JASA* **111** 1548&ndash;1563](https://doi.org/10.1080/01621459.2016.1180986)
